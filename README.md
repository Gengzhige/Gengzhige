## 欢迎来到星河AI研究院 👋

    AI时代，普通人怎么赢？


秉持“**让AI不再难学，让科研不再难搞，让求学就业不走弯路**”的初心，梗直哥团队致力于专业人工智能的知识普及和科技创新。

三年来，我们在B站、知乎等平台收获了20多万粉丝的关注与喜爱。上万名学员选修了我们人工智能三部曲等原创课程，参与了AI实战特训班、AI逐梦营等活动。数千名学员在我们的帮助下打破出身、专业、学校、地域、国家等限制，实现跨专业攻读硕博、出国留学、高薪就业、创业等目标。有志者事竟成，敢想敢干、会想会干，逆天改命就不是梦！

我们发起成了**星河AI研究院（Galaxy AI Research Academy）**。借助多年的行业积累、资源与人脉，与中美日顶尖大学、研究机构和企业开展合作，让普通学生也能无门槛地接触最新技术和高水平科研，发表高质量原创学术论文、专利和成果。每周内部交流、专题分享、线上线下活动、1对1指导。也许你不曾想到还有这样的机会，数百成员已然受惠，你动心了吗？

“**道阻且长，行则将至，行而不辍，未来可期**”。我们始终认为，梗直既是一种精神，也是人生的态度。希望我们在你奋力前行的路上成为良师益友，为你助力，为你加油，为你喝彩！

### 论文精读系列

我们近期会逐步分享强化学习领域的高引论文，欢迎关注。

[点击查看](https://space.bilibili.com/1921388479)

<table>
    <thead><tr><th>年份</th><th>名称</th><th>简介</th><th>引用</th></tr></thead>
    <tbody>
        <tr><td colspan="4">👇️DQN系列</td></tr>
        <tr><td>2013</td><td><a href='https://arxiv.org/abs/1312.5602' target='_blank'>DQN</a></td><td>首次使用深度网络结合Q-learning学习控制策略</td><td><a href='https://www.semanticscholar.org/paper/Playing-Atari-with-Deep-Reinforcement-Learning-Mnih-Kavukcuoglu/2319a491378867c7049b3da055c5df60e1671158' target='_blank'><img alt='Dynamic JSON Badge' src='https://img.shields.io/badge/dynamic/json?query=citationCount&label=citation&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F2319a491378867c7049b3da055c5df60e1671158%253Ffields%253DcitationCount'></a></td></tr>
        <tr><td>2015</td><td><a href='https://arxiv.org/abs/1509.06461' target='_blank'>DDQN</a></td><td>有效降低DQN在Atari环境中对动作价值的过估</td><td><a href='https://www.semanticscholar.org/paper/Deep-Reinforcement-Learning-with-Double-Q-Learning-Hasselt-Guez/3b9732bb07dc99bde5e1f9f75251c6ea5039373e' target='_blank'><img alt='Dynamic JSON Badge' src='https://img.shields.io/badge/dynamic/json?query=citationCount&label=citation&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F3b9732bb07dc99bde5e1f9f75251c6ea5039373e%253Ffields%253DcitationCount'></a></td></tr>
        <tr><td>2016</td><td><a href='https://arxiv.org/abs/1511.06581' target='_blank'>Dualing DQN</a></td><td>动作值函数拆分成状态值和动作优势</td><td><a href='https://www.semanticscholar.org/paper/Dueling-Network-Architectures-for-Deep-Learning-Wang-Schaul/4c05d7caa357148f0bbd61720bdd35f0bc05eb81' target='_blank'><img alt='Dynamic JSON Badge' src='https://img.shields.io/badge/dynamic/json?query=citationCount&label=citation&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F4c05d7caa357148f0bbd61720bdd35f0bc05eb81%253Ffields%253DcitationCount'></a></td></tr>
        <tr><td colspan="4">👇️策略梯度方法</td></tr>
        <tr><td>2015</td><td><a href='https://arxiv.org/abs/1509.02971' target='_blank'>DDPG</a></td><td>将确定性策略梯度与深度网络结合</td><td><a href='https://www.semanticscholar.org/paper/Continuous-control-with-deep-reinforcement-learning-Lillicrap-Hunt/024006d4c2a89f7acacc6e4438d156525b60a98f' target='_blank'><img alt='Dynamic JSON Badge' src='https://img.shields.io/badge/dynamic/json?query=citationCount&label=citation&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F024006d4c2a89f7acacc6e4438d156525b60a98f%253Ffields%253DcitationCount'></a></td></tr>
        <tr><td>2018</td><td><a href='https://arxiv.org/abs/1802.09477' target='_blank'>TD3</a></td><td>引入延迟策略更新等机制进一步优化DDPG</td><td><a href='https://www.semanticscholar.org/paper/Addressing-Function-Approximation-Error-in-Methods-Fujimoto-Hoof/4debb99c0c63bfaa97dd433bc2828e4dac81c48b' target='_blank'><img alt='Dynamic JSON Badge' src='https://img.shields.io/badge/dynamic/json?query=citationCount&label=citation&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F4debb99c0c63bfaa97dd433bc2828e4dac81c48b%253Ffields%253DcitationCount'></a></td></tr>
        <tr><td colspan="4">👇️经典Actor-Critic方法</td></tr>
        <tr><td>2016</td><td><a href='https://arxiv.org/abs/1602.01783' target='_blank'>A3C</a></td><td>异步并行架构加速训练</td><td><a href='https://www.semanticscholar.org/paper/Asynchronous-Methods-for-Deep-Reinforcement-Mnih-Badia/69e76e16740ed69f4dc55361a3d319ac2f1293dd' target='_blank'><img alt='Dynamic JSON Badge' src='https://img.shields.io/badge/dynamic/json?query=citationCount&label=citation&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F69e76e16740ed69f4dc55361a3d319ac2f1293dd%253Ffields%253DcitationCount'></a></td></tr>
        <tr><td>2018</td><td><a href='https://arxiv.org/abs/1801.01290' target='_blank'>SAC</a></td><td>基于最大熵的策略迭代框架</td><td><a href='https://www.semanticscholar.org/paper/Soft-Actor-Critic%3A-Off-Policy-Maximum-Entropy-Deep-Haarnoja-Zhou/811df72e210e20de99719539505da54762a11c6d' target='_blank'><img alt='Dynamic JSON Badge' src='https://img.shields.io/badge/dynamic/json?query=citationCount&label=citation&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F811df72e210e20de99719539505da54762a11c6d%253Ffields%253DcitationCount'></a></td></tr>
        <tr><td colspan="4">👇️PPO系列</td></tr>
        <tr><td>2015</td><td><a href='https://arxiv.org/abs/1502.05477' target='_blank'>TRPO</a></td><td>引入信赖域约束策略更新</td><td><a href='https://www.semanticscholar.org/paper/Trust-Region-Policy-Optimization-Schulman-Levine/449532187c94af3dd3aa55e16d2c50f7854d2199' target='_blank'><img alt='Dynamic JSON Badge' src='https://img.shields.io/badge/dynamic/json?query=citationCount&label=citation&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F449532187c94af3dd3aa55e16d2c50f7854d2199%253Ffields%253DcitationCount'></a></td></tr>
        <tr><td>2017</td><td><a href='https://arxiv.org/abs/1707.06347' target='_blank'>PPO</a></td><td>最广泛应用的强化学习方法之一</td><td><a href='https://www.semanticscholar.org/paper/Proximal-Policy-Optimization-Algorithms-Schulman-Wolski/dce6f9d4017b1785979e7520fd0834ef8cf02f4b' target='_blank'><img alt='Dynamic JSON Badge' src='https://img.shields.io/badge/dynamic/json?query=citationCount&label=citation&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdce6f9d4017b1785979e7520fd0834ef8cf02f4b%253Ffields%253DcitationCount'></a></td></tr>
        <tr><td>2023</td><td><a href='https://arxiv.org/abs/2305.18290' target='_blank'>DPO</a></td><td>基于偏好数据隐式学习奖励模型</td><td><a href='https://www.semanticscholar.org/paper/Direct-Preference-Optimization%3A-Your-Language-Model-Rafailov-Sharma/0d1c76d45afa012ded7ab741194baf142117c495' target='_blank'><img alt='Dynamic JSON Badge' src='https://img.shields.io/badge/dynamic/json?query=citationCount&label=citation&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0d1c76d45afa012ded7ab741194baf142117c495%253Ffields%253DcitationCount'></a></td></tr>
        <tr><td>2024</td><td><a href='https://arxiv.org/abs/2402.03300' target='_blank'>GRPO</a></td><td>根据群体得分估计基线</td><td><a href='https://www.semanticscholar.org/paper/DeepSeekMath%3A-Pushing-the-Limits-of-Mathematical-in-Shao-Wang/35b142ea69598e6241f0011312128031df55895c' target='_blank'><img alt='Dynamic JSON Badge' src='https://img.shields.io/badge/dynamic/json?query=citationCount&label=citation&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F35b142ea69598e6241f0011312128031df55895c%253Ffields%253DcitationCount'></a></td></tr>
        <tr><td>2025</td><td><a href='https://arxiv.org/abs/2503.14476' target='_blank'>DAPO</a></td><td>引入解耦裁剪、动态采样等机制</td><td><a href='https://www.semanticscholar.org/paper/DAPO%3A-An-Open-Source-LLM-Reinforcement-Learning-at-Yu-Zhang/dd4cfde3e135f799a9a71b4f57e13a29de89f7e3' target='_blank'><img alt='Dynamic JSON Badge' src='https://img.shields.io/badge/dynamic/json?query=citationCount&label=citation&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdd4cfde3e135f799a9a71b4f57e13a29de89f7e3%253Ffields%253DcitationCount'></a></td></tr>
        <tr><td colspan="4">👇️多智能体</td></tr>
        <tr><td>2017</td><td><a href='https://arxiv.org/abs/1706.02275' target='_blank'>MADDPG</a></td><td>多智能体环境实现集中式Critic和分散式Actor架构</td><td><a href='https://www.semanticscholar.org/paper/Multi-Agent-Actor-Critic-for-Mixed-Environments-Lowe-Wu/7c3ece1ba41c415d7e81cfa5ca33a8de66efd434' target='_blank'><img alt='Dynamic JSON Badge' src='https://img.shields.io/badge/dynamic/json?query=citationCount&label=citation&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F7c3ece1ba41c415d7e81cfa5ca33a8de66efd434%253Ffields%253DcitationCount'></a></td></tr>
        <tr><td>2021</td><td><a href='https://arxiv.org/abs/2103.01955' target='_blank'>MAPPO</a></td><td>PPO在多智能体环境同样可以取得良好效果</td><td><a href='https://www.semanticscholar.org/paper/The-Surprising-Effectiveness-of-PPO-in-Cooperative-Yu-Velu/3a315c81a98851f0614c09fef6a14c30d6a1e63c' target='_blank'><img alt='Dynamic JSON Badge' src='https://img.shields.io/badge/dynamic/json?query=citationCount&label=citation&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F3a315c81a98851f0614c09fef6a14c30d6a1e63c%253Ffields%253DcitationCount'></a></td></tr>
    </tbody>
</table>

### 科研项目 - 星河计划招生简章

梗直哥招学生了！因会亲力亲为，希望把最好的指导和资源给到最需要的同学，本着宁缺毋滥的原则，每期仅严选10-15人（精英项目+新星项目）。条件如下：
1. 25-36岁对AI科研兴趣强烈，希望深造、转型、转行、留学、基金申请、创业等。
2. 本科以上学历，专业不限，学校不限。事实上已有学生中一半以上为其他专业背景，“X+AI”模式，我们相信跨领域合作才是AI发展的未来。从双非到top3，不论出身，只看动力和能力，我们希望在传统培养模式之外提供有志之才独特的成长与创新路径。
3. 单身有工作经历者优先，大厂小厂无所谓，体制内外都可以，什么行业都行。
4. 有博士学历或科研经历者优先，大学或科研机构年轻教师优先，有留学经历优先。
5. 已加入星河AI研究院成员优先。

我们目前与中科院、京大、伊利诺伊等国内外顶尖科研机构，谷歌、微软、腾讯等一线大厂建立了良好的合作与联合培养机制。在研项目覆盖AI Agent，具身智能（机器人、自动驾驶），AI4S（投资、医学、计算化学、生物）等领域。

如果你在科研领域苦苦挣扎不知发展方向，如果你职场遭遇瓶颈渴望转型，如果你导师放养前途迷茫，这里也许是你寻求人生突破实现逆袭的最好机会！

- **精英项目**：为期10-12个月，定制化培养模式，全程1对1指导，配备专业导师和合作资源。适合有一定实力和想法，希望能迫切改变当前处境，无论在学术上、职业发展上实现转型和跨越式发展的同学（痛苦指数8-10）。比如工作出现瓶颈、面临内卷的年轻老师、行业坍塌亟需转型等情况。从选题、调研、课题规划、算法设计、实验、输出全流程个性化指导。让你借助科研成果做敲门砖，砸开下一个机会！

- **新星项目**：为期3-6个月，参与既有项目，在老师指导下实现对AI科研过程的熟悉。适合实力有限，但又想快速提升科研经验，转型算法工程师或早日拿到结果毕业的同学（痛苦指数6-7）。比如程序员群体、刚本科毕业或者研一学生等。

- **科研加速营**：为期1个月，针对最难的选题选方向环节，从AI思维、核心算法、学习路线设计、学术职业规划等角度突击培训。适合对科研前景充满期待但又深感迷茫的同学（痛苦指数4-5），比如在校学生、职场人士，有一定想法但还没有确切行动意愿。20课时录播+大量内部材料+8课时直播+1次个性化咨询。正价￥6k，内部早鸟价格￥3k。导师除梗直哥团队外，还包括京都大学等知名院所教授、腾讯等一线国内外大厂资深技术专家、team leader专题讲座和面对面交流。

**有意者请将简历发至微信gengzhige99，所有申请都会回复。我们将近期择优安排线上或线下面试。**

